apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnector
metadata:
  # connector name
  name: snapshot-src-postgresql-cdc-avro-c3118a76
  labels: 
    # kafka connect [cluster] name
    strimzi.io/cluster: connect-cluster
spec:
  class: io.debezium.connector.postgresql.PostgresConnector
  tasksMax: 2 
  config:
    ### Postgres configuration ###
    plugin.name: "pgoutput"
    slot.name: "debezium_snapshot"
    publication.name: dbz_publication

    ### Schema and avro configuration ###
    key.converter: "io.confluent.connect.avro.AvroConverter"
    key.converter.schema.registry.url: "http://schema-registry-cp-schema-registry:8081"
    value.converter: "io.confluent.connect.avro.AvroConverter"
    value.converter.schema.registry.url: "http://schema-registry-cp-schema-registry:8081"

    ### Credential configurations ###
    database.hostname: "${file:/opt/kafka/external-configuration/connector-config-postgresql/connector.properties:postgres_hostname}"
    database.port: "${file:/opt/kafka/external-configuration/connector-config-postgresql/connector.properties:postgres_port}"
    database.user: "${file:/opt/kafka/external-configuration/connector-config-postgresql/connector.properties:postgres_username}"
    database.password: "${file:/opt/kafka/external-configuration/connector-config-postgresql/connector.properties:postgres_password}"
    database.dbname: "${file:/opt/kafka/external-configuration/connector-config-postgresql/connector.properties:postgres_dbname}"

    ### Initial snapshot operation ###
    snapshot.mode: initial_only
    # snapshot.fetch.size: 50000
    # incremental.snapshot.chunk.size: 10000

    #### CDC operation ###
    max.batch.size: 50000
    max.queue.size: 60000
    poll.interval.ms: 100

    #### Table and primary key ###
    table.include.list: public.table_one,public.table_two
    message.key.columns: "public.table_one:id_table_one;public.table_two:id_table_two"
    topic.prefix: postgres

    ### Handling events ###
    time.precision.mode: connect
    tombstones.onâ€‹.delete: true
    decimal.handling.mode: double

    ### SMT - Simple Message Transformation ###
    transforms: "unwrap,tsConverter1,tsConverter2,tsConverter3"

    transforms.unwrap.type: io.debezium.transforms.ExtractNewRecordState
    transforms.unwrap.drop.tombstones: false
    transforms.unwrap.delete.handling.mode: rewrite
    transforms.unwrap.add.fields: db,schema,table,lsn,ts_ms,op

    transforms.tsConverter1.type: "org.apache.kafka.connect.transforms.TimestampConverter$Value"
    transforms.tsConverter1.format: "yyyy-MM-dd HH:mm:ss.SSS"
    transforms.tsConverter1.target.type: "string"
    transforms.tsConverter1.field: "__ts_ms"

    transforms.tsConverter2.type: "org.apache.kafka.connect.transforms.TimestampConverter$Value"
    transforms.tsConverter2.format: "yyyy-MM-dd HH:mm:ss.SSS"
    transforms.tsConverter2.target.type: "string"
    transforms.tsConverter2.field: "timestamp_field"

    transforms.tsConverter3.type: "org.apache.kafka.connect.transforms.TimestampConverter$Value"
    transforms.tsConverter3.format: "yyyy-MM-dd"
    transforms.tsConverter3.target.type: "string"
    transforms.tsConverter3.field: "date_field"