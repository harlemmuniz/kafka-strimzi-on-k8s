apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnector
metadata:
  # connector name
  name: "sink-sqlserver-addresses-json-7661b667"
  labels:
    # kafka connect [cluster] name
    strimzi.io/cluster: connect-cluster
spec:
  class: io.confluent.connect.jdbc.JdbcSinkConnector
  tasksMax: 1
  config:
    key.converter: org.apache.kafka.connect.json.JsonConverter
    value.converter: org.apache.kafka.connect.json.JsonConverter
    key.converter.schemas.enable: false
    value.converter.schemas.enable: false
    connection.url: "jdbc:sqlserver://35.188.86.122:1433;databaseName=kafka_test"
    connection.user: "sqlserver"
    connection.password: "sqlserver"
    connection.attempts: "1"
    topics: "src-sqlserver-addresses-json"
    table.name.format: "enriched_addresses" # nome da tabela caso o connector for criar
    auto.create: "false" # criar tabela se não existir
    auto.evolve: "false" # permitir schema evolution
    insert.mode: "insert" # modo apenas inserção (upsert faz update e insert)
    pk.mode: "none" # usar a chave que está dentro do value - corpo da mensagem (record_value) ou da key - header da mensagem (record_key)
    pk.fields: "none"  # informar o nome da coluna
    batch.size: "1500" # agrupar 1500 mensagens em um lote, antes de enviar
    delete.enabled: False # se não informar pk.mode e pk.fields, tem que desabilitar o delete, pois é baseado na chave