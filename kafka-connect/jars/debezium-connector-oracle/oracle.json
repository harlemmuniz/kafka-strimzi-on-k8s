{
  "openapi" : "3.0.3",
  "info" : {
    "title" : "Generated by Debezium OpenAPI Generator"
  },
  "components" : {
    "schemas" : {
      "debezium-oracle-2.1.1.Final" : {
        "title" : "Debezium Oracle Connector",
        "required" : [ "topic.prefix", "database.hostname", "database.user", "database.dbname" ],
        "type" : "object",
        "properties" : {
          "topic.prefix" : {
            "title" : "Topic prefix",
            "description" : "Topic prefix that identifies and provides a namespace for the particular database server/cluster is capturing changes. The topic prefix should be unique across all other connectors, since it is used as a prefix for all Kafka topic names that receive events emitted by this connector. Only alphanumeric characters, hyphens, dots and underscores must be accepted.",
            "type" : "string",
            "nullable" : false,
            "x-name" : "topic.prefix",
            "x-category" : "CONNECTION"
          },
          "database.hostname" : {
            "title" : "Hostname",
            "description" : "Resolvable hostname or IP address of the database server.",
            "type" : "string",
            "nullable" : false,
            "x-name" : "database.hostname",
            "x-category" : "CONNECTION"
          },
          "database.port" : {
            "format" : "int32",
            "title" : "Port",
            "description" : "Port of the database server.",
            "default" : 1528,
            "type" : "integer",
            "x-name" : "database.port",
            "x-category" : "CONNECTION"
          },
          "database.user" : {
            "title" : "User",
            "description" : "Name of the database user to be used when connecting to the database.",
            "type" : "string",
            "nullable" : false,
            "x-name" : "database.user",
            "x-category" : "CONNECTION"
          },
          "database.password" : {
            "format" : "password",
            "title" : "Password",
            "description" : "Password of the database user to be used when connecting to the database.",
            "type" : "string",
            "x-name" : "database.password",
            "x-category" : "CONNECTION"
          },
          "database.dbname" : {
            "title" : "Database",
            "description" : "The name of the database from which the connector should capture changes",
            "type" : "string",
            "nullable" : false,
            "x-name" : "database.dbname",
            "x-category" : "CONNECTION"
          },
          "database.pdb.name" : {
            "title" : "PDB name",
            "description" : "Name of the pluggable database when working with a multi-tenant set-up. The CDB name must be given via database.dbname in this case.",
            "type" : "string",
            "x-name" : "database.pdb.name",
            "x-category" : "CONNECTION"
          },
          "database.out.server.name" : {
            "title" : "XStream out server name",
            "description" : "Name of the XStream Out server to connect to.",
            "type" : "string",
            "x-name" : "database.out.server.name",
            "x-category" : "CONNECTION"
          },
          "database.url" : {
            "title" : "Complete JDBC URL",
            "description" : "Complete JDBC URL as an alternative to specifying hostname, port and database provided as a way to support alternative connection scenarios.",
            "type" : "string",
            "x-name" : "database.url",
            "x-category" : "CONNECTION"
          },
          "rac.nodes" : {
            "title" : "Oracle RAC nodes",
            "description" : "A comma-separated list of RAC node hostnames or ip addresses",
            "type" : "string",
            "x-name" : "rac.nodes",
            "x-category" : "CONNECTION"
          },
          "schema.history.internal.kafka.bootstrap.servers" : {
            "title" : "Kafka broker addresses",
            "description" : "A list of host/port pairs that the connector will use for establishing the initial connection to the Kafka cluster for retrieving database schema history previously stored by the connector. This should point to the same Kafka cluster used by the Kafka Connect process.",
            "type" : "string",
            "x-name" : "schema.history.internal.kafka.bootstrap.servers",
            "x-category" : "CONNECTION"
          },
          "schema.history.internal.kafka.topic" : {
            "title" : "Database schema history topic name",
            "description" : "The name of the topic for the database schema history",
            "type" : "string",
            "x-name" : "schema.history.internal.kafka.topic",
            "x-category" : "CONNECTION"
          },
          "schema.history.internal.kafka.query.timeout.ms" : {
            "format" : "int64",
            "title" : "Kafka admin client query timeout (ms)",
            "description" : "The number of milliseconds to wait while fetching cluster information using Kafka admin client.",
            "default" : 3000,
            "type" : "integer",
            "x-name" : "schema.history.internal.kafka.query.timeout.ms",
            "x-category" : "CONNECTION"
          },
          "database.connection.adapter" : {
            "title" : "Connector adapter",
            "description" : "The adapter to use when capturing changes from the database. Options include: 'logminer': (the default) to capture changes using native Oracle LogMiner; 'xstream' to capture changes using Oracle XStreams",
            "default" : "LogMiner",
            "enum" : [ "xstream", "logminer" ],
            "type" : "string",
            "x-name" : "database.connection.adapter",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.strategy" : {
            "title" : "Log Mining Strategy",
            "description" : "There are strategies: Online catalog with faster mining but no captured DDL. Another - with data dictionary loaded into REDO LOG files",
            "default" : "redo_log_catalog",
            "enum" : [ "redo_log_catalog", "online_catalog" ],
            "type" : "string",
            "x-name" : "log.mining.strategy",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.archive.log.only.mode" : {
            "title" : "Specifies whether log mining should only target archive logs or both archive and redo logs",
            "description" : "When set to 'false', the default, the connector will mine both archive log and redo logs to emit change events. When set to 'true', the connector will only mine archive logs. There are circumstances where its advantageous to only mine archive logs and accept latency in event emission due to frequent revolving redo logs.",
            "default" : false,
            "type" : "boolean",
            "x-name" : "log.mining.archive.log.only.mode",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.archive.log.hours" : {
            "format" : "int64",
            "title" : "Log Mining Archive Log Hours",
            "description" : "The number of hours in the past from SYSDATE to mine archive logs. Using 0 mines all available archive logs",
            "default" : 0,
            "type" : "integer",
            "x-name" : "log.mining.archive.log.hours",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.batch.size.default" : {
            "format" : "int64",
            "title" : "Default batch size for reading redo/archive logs.",
            "description" : "The starting SCN interval size that the connector will use for reading data from redo/archive logs.",
            "default" : 20000,
            "type" : "integer",
            "x-name" : "log.mining.batch.size.default",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.batch.size.min" : {
            "format" : "int64",
            "title" : "Minimum batch size for reading redo/archive logs.",
            "description" : "The minimum SCN interval size that this connector will try to read from redo/archive logs. Active batch size will be also increased/decreased by this amount for tuning connector throughput when needed.",
            "default" : 1000,
            "type" : "integer",
            "x-name" : "log.mining.batch.size.min",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.batch.size.max" : {
            "format" : "int64",
            "title" : "Maximum batch size for reading redo/archive logs.",
            "description" : "The maximum SCN interval size that this connector will use when reading from redo/archive logs.",
            "default" : 100000,
            "type" : "integer",
            "x-name" : "log.mining.batch.size.max",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.sleep.time.default.ms" : {
            "format" : "int64",
            "title" : "Default sleep time in milliseconds when reading redo/archive logs.",
            "description" : "The amount of time that the connector will sleep after reading data from redo/archive logs and before starting reading data again. Value is in milliseconds.",
            "default" : 1000,
            "type" : "integer",
            "x-name" : "log.mining.sleep.time.default.ms",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.sleep.time.min.ms" : {
            "format" : "int64",
            "title" : "Minimum sleep time in milliseconds when reading redo/archive logs.",
            "description" : "The minimum amount of time that the connector will sleep after reading data from redo/archive logs and before starting reading data again. Value is in milliseconds.",
            "default" : 0,
            "type" : "integer",
            "x-name" : "log.mining.sleep.time.min.ms",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.sleep.time.max.ms" : {
            "format" : "int64",
            "title" : "Maximum sleep time in milliseconds when reading redo/archive logs.",
            "description" : "The maximum amount of time that the connector will sleep after reading data from redo/archive logs and before starting reading data again. Value is in milliseconds.",
            "default" : 3000,
            "type" : "integer",
            "x-name" : "log.mining.sleep.time.max.ms",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.sleep.time.increment.ms" : {
            "format" : "int64",
            "title" : "The increment in sleep time in milliseconds used to tune auto-sleep behavior.",
            "description" : "The maximum amount of time that the connector will use to tune the optimal sleep time when reading data from LogMiner. Value is in milliseconds.",
            "default" : 200,
            "type" : "integer",
            "x-name" : "log.mining.sleep.time.increment.ms",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.transaction.retention.hours" : {
            "format" : "int64",
            "title" : "Log Mining long running transaction retention",
            "description" : "Hours to keep long running transactions in transaction buffer between log mining sessions. By default, all transactions are retained.",
            "default" : 0,
            "type" : "integer",
            "x-name" : "log.mining.transaction.retention.hours",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.username.exclude.list" : {
            "title" : "List of users to exclude from LogMiner query",
            "description" : "Comma separated list of usernames to exclude from LogMiner query.",
            "type" : "string",
            "x-name" : "log.mining.username.exclude.list",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.archive.destination.name" : {
            "title" : "Name of the archive log destination to be used for reading archive logs",
            "description" : "Sets the specific archive log destination as the source for reading archive logs.When not set, the connector will automatically select the first LOCAL and VALID destination.",
            "type" : "string",
            "x-name" : "log.mining.archive.destination.name",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.buffer.type" : {
            "title" : "Controls which buffer type implementation to be used",
            "description" : "The buffer type controls how the connector manages buffering transaction data.\n\nmemory - Uses the JVM process' heap to buffer all transaction data.\n\ninfinispan_embedded - This option uses an embedded Infinispan cache to buffer transaction data and persist it to disk.\n\ninfinispan_remote - This option uses a remote Infinispan cluster to buffer transaction data and persist it to disk.",
            "default" : "memory",
            "enum" : [ "memory", "infinispan_embedded", "infinispan_remote" ],
            "type" : "string",
            "x-name" : "log.mining.buffer.type",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.buffer.infinispan.cache.transactions" : {
            "title" : "Infinispan 'transactions' cache configuration",
            "description" : "Specifies the XML configuration for the Infinispan 'transactions' cache",
            "type" : "string",
            "x-name" : "log.mining.buffer.infinispan.cache.transactions",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.buffer.infinispan.cache.events" : {
            "title" : "Infinispan 'events' cache configurations",
            "description" : "Specifies the XML configuration for the Infinispan 'events' cache",
            "type" : "string",
            "x-name" : "log.mining.buffer.infinispan.cache.events",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.buffer.infinispan.cache.processed_transactions" : {
            "title" : "Infinispan 'processed-transactions' cache configuration",
            "description" : "Specifies the XML configuration for the Infinispan 'processed-transactions' cache",
            "type" : "string",
            "x-name" : "log.mining.buffer.infinispan.cache.processed_transactions",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.buffer.infinispan.cache.schema_changes" : {
            "title" : "Infinispan 'schema-changes' cache configuration",
            "description" : "Specifies the XML configuration for the Infinispan 'schema-changes' cache",
            "type" : "string",
            "x-name" : "log.mining.buffer.infinispan.cache.schema_changes",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.archive.log.only.scn.poll.interval.ms" : {
            "format" : "int64",
            "title" : "The interval in milliseconds to wait between polls when SCN is not yet in the archive logs",
            "description" : "The interval in milliseconds to wait between polls checking to see if the SCN is in the archive logs.",
            "default" : 10000,
            "type" : "integer",
            "x-name" : "log.mining.archive.log.only.scn.poll.interval.ms",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.scn.gap.detection.gap.size.min" : {
            "format" : "int64",
            "title" : "SCN gap size used to detect SCN gap",
            "description" : "Used for SCN gap detection, if the difference between current SCN and previous end SCN is bigger than this value, and the time difference of current SCN and previous end SCN is smaller than log.mining.scn.gap.detection.time.interval.max.ms, consider it a SCN gap.",
            "default" : 1000000,
            "type" : "integer",
            "x-name" : "log.mining.scn.gap.detection.gap.size.min",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "log.mining.scn.gap.detection.time.interval.max.ms" : {
            "format" : "int64",
            "title" : "Timer interval used to detect SCN gap",
            "description" : "Used for SCN gap detection, if the difference between current SCN and previous end SCN is bigger than log.mining.scn.gap.detection.gap.size.min, and the time difference of current SCN and previous end SCN is smaller than  this value, consider it a SCN gap.",
            "default" : 20000,
            "type" : "integer",
            "x-name" : "log.mining.scn.gap.detection.time.interval.max.ms",
            "x-category" : "CONNECTION_ADVANCED"
          },
          "table.include.list" : {
            "format" : "list,regex",
            "title" : "Include Tables",
            "description" : "The tables for which changes are to be captured",
            "type" : "string",
            "x-name" : "table.include.list",
            "x-category" : "FILTERS"
          },
          "table.exclude.list" : {
            "format" : "list,regex",
            "title" : "Exclude Tables",
            "description" : "A comma-separated list of regular expressions that match the fully-qualified names of tables to be excluded from monitoring",
            "type" : "string",
            "x-name" : "table.exclude.list",
            "x-category" : "FILTERS"
          },
          "column.include.list" : {
            "format" : "list,regex",
            "title" : "Include Columns",
            "description" : "Regular expressions matching columns to include in change events",
            "type" : "string",
            "x-name" : "column.include.list",
            "x-category" : "FILTERS"
          },
          "column.exclude.list" : {
            "format" : "list,regex",
            "title" : "Exclude Columns",
            "description" : "Regular expressions matching columns to exclude from change events",
            "type" : "string",
            "x-name" : "column.exclude.list",
            "x-category" : "FILTERS"
          },
          "snapshot.mode" : {
            "title" : "Snapshot mode",
            "description" : "The criteria for running a snapshot upon startup of the connector. Options include: 'initial' (the default) to specify the connector should run a snapshot only when no offsets are available for the logical server name; 'schema_only' to specify the connector should run a snapshot of the schema when no offsets are available for the logical server name. ",
            "default" : "initial",
            "enum" : [ "always", "initial_only", "initial", "schema_only", "schema_only_recovery" ],
            "type" : "string",
            "x-name" : "snapshot.mode",
            "x-category" : "CONNECTOR_SNAPSHOT"
          },
          "snapshot.locking.mode" : {
            "title" : "Snapshot locking mode",
            "description" : "Controls how the connector holds locks on tables while performing the schema snapshot. The default is 'shared', which means the connector will hold a table lock that prevents exclusive table access for just the initial portion of the snapshot while the database schemas and other metadata are being read. The remaining work in a snapshot involves selecting all rows from each table, and this is done using a flashback query that requires no locks. However, in some cases it may be desirable to avoid locks entirely which can be done by specifying 'none'. This mode is only safe to use if no schema changes are happening while the snapshot is taken.",
            "default" : "shared",
            "enum" : [ "shared", "none" ],
            "type" : "string",
            "x-name" : "snapshot.locking.mode",
            "x-category" : "CONNECTOR_SNAPSHOT"
          },
          "snapshot.include.collection.list" : {
            "format" : "list,regex",
            "title" : "Snapshot mode include data collection",
            "description" : "This setting must be set to specify a list of tables/collections whose snapshot must be taken on creating or restarting the connector.",
            "type" : "string",
            "x-name" : "snapshot.include.collection.list",
            "x-category" : "CONNECTOR_SNAPSHOT"
          },
          "snapshot.fetch.size" : {
            "format" : "int32",
            "title" : "Snapshot fetch size",
            "description" : "The maximum number of records that should be loaded into memory while performing a snapshot.",
            "type" : "integer",
            "x-name" : "snapshot.fetch.size",
            "x-category" : "CONNECTOR_SNAPSHOT"
          },
          "snapshot.delay.ms" : {
            "format" : "int64",
            "title" : "Snapshot Delay (milliseconds)",
            "description" : "A delay period before a snapshot will begin, given in milliseconds. Defaults to 0 ms.",
            "default" : 0,
            "type" : "integer",
            "x-name" : "snapshot.delay.ms",
            "x-category" : "CONNECTOR_SNAPSHOT"
          },
          "snapshot.lock.timeout.ms" : {
            "format" : "int64",
            "title" : "Snapshot lock timeout (ms)",
            "description" : "The maximum number of millis to wait for table locks at the beginning of a snapshot. If locks cannot be acquired in this time frame, the snapshot will be aborted. Defaults to 10 seconds",
            "default" : 10000,
            "type" : "integer",
            "x-name" : "snapshot.lock.timeout.ms",
            "x-category" : "CONNECTOR_SNAPSHOT"
          },
          "snapshot.max.threads" : {
            "format" : "int32",
            "title" : "Snapshot maximum threads",
            "description" : "The maximum number of threads used to perform the snapshot. Defaults to 1.",
            "default" : 1,
            "type" : "integer",
            "x-name" : "snapshot.max.threads",
            "x-category" : "CONNECTOR_SNAPSHOT"
          },
          "snapshot.select.statement.overrides" : {
            "title" : "List of tables where the default select statement used during snapshotting should be overridden.",
            "description" : " This property contains a comma-separated list of fully-qualified tables (DB_NAME.TABLE_NAME) or (SCHEMA_NAME.TABLE_NAME), depending on the specific connectors. Select statements for the individual tables are specified in further configuration properties, one for each table, identified by the id 'snapshot.select.statement.overrides.[DB_NAME].[TABLE_NAME]' or 'snapshot.select.statement.overrides.[SCHEMA_NAME].[TABLE_NAME]', respectively. The value of those properties is the select statement to use when retrieving data from the specific table during snapshotting. A possible use case for large append-only tables is setting a specific point where to start (resume) snapshotting, in case a previous snapshotting was interrupted.",
            "type" : "string",
            "x-name" : "snapshot.select.statement.overrides",
            "x-category" : "CONNECTOR_SNAPSHOT"
          },
          "snapshot.enhance.predicate.scn" : {
            "title" : "A string to replace on snapshot predicate enhancement",
            "description" : "A token to replace on snapshot predicate template",
            "type" : "string",
            "x-name" : "snapshot.enhance.predicate.scn",
            "x-category" : "CONNECTOR_SNAPSHOT"
          },
          "include.schema.changes" : {
            "title" : "Include database schema changes",
            "description" : "Whether the connector should publish changes in the database schema to a Kafka topic with the same name as the database server ID. Each schema change will be recorded using a key that contains the database name and whose value include logical description of the new schema and optionally the DDL statement(s). The default is 'true'. This is independent of how the connector internally records database schema history.",
            "default" : true,
            "type" : "boolean",
            "x-name" : "include.schema.changes",
            "x-category" : "CONNECTOR"
          },
          "tombstones.on.delete" : {
            "title" : "Change the behaviour of Debezium with regards to delete operations",
            "description" : "Whether delete operations should be represented by a delete event and a subsequent tombstone event (true) or only by a delete event (false). Emitting the tombstone event (the default behavior) allows Kafka to completely delete all events pertaining to the given key once the source record got deleted.",
            "default" : true,
            "type" : "boolean",
            "x-name" : "tombstones.on.delete",
            "x-category" : "CONNECTOR"
          },
          "decimal.handling.mode" : {
            "title" : "Decimal Handling",
            "description" : "Specify how DECIMAL and NUMERIC columns should be represented in change events, including: 'precise' (the default) uses java.math.BigDecimal to represent values, which are encoded in the change events using a binary representation and Kafka Connect's 'org.apache.kafka.connect.data.Decimal' type; 'string' uses string to represent values; 'double' represents values using Java's 'double', which may not offer the precision but will be far easier to use in consumers.",
            "default" : "precise",
            "enum" : [ "string", "double", "precise" ],
            "type" : "string",
            "x-name" : "decimal.handling.mode",
            "x-category" : "CONNECTOR"
          },
          "binary.handling.mode" : {
            "title" : "Binary Handling",
            "description" : "Specify how binary (blob, binary, etc.) columns should be represented in change events, including: 'bytes' represents binary data as byte array (default); 'base64' represents binary data as base64-encoded string; 'base64-url-safe' represents binary data as base64-url-safe-encoded string; 'hex' represents binary data as hex-encoded (base16) string",
            "default" : "bytes",
            "enum" : [ "bytes", "base64", "hex", "base64-url-safe" ],
            "type" : "string",
            "x-name" : "binary.handling.mode",
            "x-category" : "CONNECTOR"
          },
          "time.precision.mode" : {
            "title" : "Time Precision",
            "description" : "Time, date, and timestamps can be represented with different kinds of precisions, including: 'adaptive' (the default) bases the precision of time, date, and timestamp values on the database column's precision; 'adaptive_time_microseconds' like 'adaptive' mode, but TIME fields always use microseconds precision; 'connect' always represents time, date, and timestamp values using Kafka Connect's built-in representations for Time, Date, and Timestamp, which uses millisecond precision regardless of the database columns' precision.",
            "default" : "adaptive",
            "enum" : [ "adaptive", "adaptive_time_microseconds", "connect" ],
            "type" : "string",
            "x-name" : "time.precision.mode",
            "x-category" : "CONNECTOR"
          },
          "include.schema.comments" : {
            "title" : "Include Table and Column Comments",
            "description" : "Whether the connector parse table and column's comment to metadata object. Note: Enable this option will bring the implications on memory usage. The number and size of ColumnImpl objects is what largely impacts how much memory is consumed by the Debezium connectors, and adding a String to each of them can potentially be quite heavy. The default is 'false'.",
            "default" : false,
            "type" : "boolean",
            "x-name" : "include.schema.comments",
            "x-category" : "CONNECTOR"
          },
          "interval.handling.mode" : {
            "title" : "Interval Handling",
            "description" : "Specify how INTERVAL columns should be represented in change events, including: 'string' represents values as an exact ISO formatted string; 'numeric' (default) represents values using the inexact conversion into microseconds",
            "default" : "numeric",
            "enum" : [ "string", "numeric" ],
            "type" : "string",
            "x-name" : "interval.handling.mode",
            "x-category" : "CONNECTOR"
          },
          "schema.name.adjustment.mode" : {
            "title" : "Schema Name Adjustment",
            "description" : "Specify how schema names should be adjusted for compatibility with the message converter used by the connector, including: 'avro' replaces the characters that cannot be used in the Avro type name with underscore; 'none' does not apply any adjustment (default)",
            "default" : "none",
            "enum" : [ "none", "avro" ],
            "type" : "string",
            "x-name" : "schema.name.adjustment.mode",
            "x-category" : "CONNECTOR"
          },
          "heartbeat.interval.ms" : {
            "format" : "int32",
            "title" : "Connector heartbeat interval (milli-seconds)",
            "description" : "Length of an interval in milli-seconds in in which the connector periodically sends heartbeat messages to a heartbeat topic. Use 0 to disable heartbeat messages. Disabled by default.",
            "default" : 0,
            "type" : "integer",
            "x-name" : "heartbeat.interval.ms",
            "x-category" : "ADVANCED_HEARTBEAT"
          },
          "heartbeat.topics.prefix" : {
            "title" : "A prefix used for naming of heartbeat topics",
            "description" : "The prefix that is used to name heartbeat topics.Defaults to __debezium-heartbeat.",
            "default" : "__debezium-heartbeat",
            "type" : "string",
            "x-name" : "heartbeat.topics.prefix",
            "x-category" : "ADVANCED_HEARTBEAT"
          },
          "heartbeat.action.query" : {
            "title" : "An optional query to execute with every heartbeat",
            "description" : "The query executed with every heartbeat.",
            "type" : "string",
            "x-name" : "heartbeat.action.query",
            "x-category" : "ADVANCED_HEARTBEAT"
          },
          "converters" : {
            "title" : "List of prefixes defining custom values converters.",
            "description" : "Optional list of custom converters that would be used instead of default ones. The converters are defined using '<converter.prefix>.type' config option and configured using options '<converter.prefix>.<option>'",
            "type" : "string",
            "x-name" : "converters",
            "x-category" : "CONNECTOR_ADVANCED"
          },
          "column.truncate.to.(d+).chars" : {
            "format" : "int32",
            "title" : "Truncate Columns To n Characters",
            "description" : "A comma-separated list of regular expressions matching fully-qualified names of columns that should be truncated to the configured amount of characters.",
            "type" : "integer",
            "x-name" : "column.truncate.to.(d+).chars",
            "x-category" : "CONNECTOR_ADVANCED"
          },
          "column.mask.with.(d+).chars" : {
            "title" : "Mask Columns With n Asterisks",
            "description" : "A comma-separated list of regular expressions matching fully-qualified names of columns that should be masked with configured amount of asterisk ('*') characters.",
            "type" : "string",
            "x-name" : "column.mask.with.(d+).chars",
            "x-category" : "CONNECTOR_ADVANCED"
          },
          "column.mask.hash.([^.]+).with.salt.(.+)" : {
            "title" : "Mask Columns Using Hash and Salt",
            "description" : "A comma-separated list of regular expressions matching fully-qualified names of columns that should be masked by hashing the input. Using the specified hash algorithms and salt.",
            "type" : "string",
            "x-name" : "column.mask.hash.([^.]+).with.salt.(.+)",
            "x-category" : "CONNECTOR_ADVANCED"
          },
          "datatype.propagate.source.type" : {
            "format" : "list,regex",
            "title" : "Propagate Source Types by Data Type",
            "description" : "A comma-separated list of regular expressions matching the database-specific data type names that adds the data type's original type and original length as parameters to the corresponding field schemas in the emitted change records.",
            "type" : "string",
            "x-name" : "datatype.propagate.source.type",
            "x-category" : "CONNECTOR_ADVANCED"
          },
          "column.propagate.source.type" : {
            "format" : "list,regex",
            "title" : "Propagate Source Types by Columns",
            "description" : "A comma-separated list of regular expressions matching fully-qualified names of columns that adds the columnâ€™s original type and original length as parameters to the corresponding field schemas in the emitted change records.",
            "type" : "string",
            "x-name" : "column.propagate.source.type",
            "x-category" : "CONNECTOR_ADVANCED"
          },
          "message.key.columns" : {
            "title" : "Columns PK mapping",
            "description" : "A semicolon-separated list of expressions that match fully-qualified tables and column(s) to be used as message key. Each expression must match the pattern '<fully-qualified table name>:<key columns>', where the table names could be defined as (DB_NAME.TABLE_NAME) or (SCHEMA_NAME.TABLE_NAME), depending on the specific connector, and the key columns are a comma-separated list of columns representing the custom key. For any table without an explicit key configuration the table's primary key column(s) will be used as message key. Example: dbserver1.inventory.orderlines:orderId,orderLineId;dbserver1.inventory.orders:id",
            "type" : "string",
            "x-name" : "message.key.columns",
            "x-category" : "CONNECTOR_ADVANCED"
          },
          "provide.transaction.metadata" : {
            "title" : "Store transaction metadata information in a dedicated topic.",
            "description" : "Enables transaction metadata extraction together with event counting",
            "default" : false,
            "type" : "boolean",
            "x-name" : "provide.transaction.metadata",
            "x-category" : "CONNECTOR_ADVANCED"
          },
          "sanitize.field.names" : {
            "title" : "Sanitize field names to adhere to Avro naming conventions",
            "description" : "Whether field names will be sanitized to Avro naming conventions",
            "default" : false,
            "type" : "boolean",
            "x-name" : "sanitize.field.names",
            "x-category" : "CONNECTOR_ADVANCED"
          },
          "unavailable.value.placeholder" : {
            "title" : "Unavailable value placeholder",
            "description" : "Specify the constant that will be provided by Debezium to indicate that the original value is unavailable and not provided by the database.",
            "default" : "__debezium_unavailable_value",
            "type" : "string",
            "x-name" : "unavailable.value.placeholder",
            "x-category" : "CONNECTOR_ADVANCED"
          },
          "lob.enabled" : {
            "title" : "Specifies whether the connector supports mining LOB fields and operations",
            "description" : "When set to 'false', the default, LOB fields will not be captured nor emitted. When set to 'true', the connector will capture LOB fields and emit changes for those fields like any other column type.",
            "default" : false,
            "type" : "boolean",
            "x-name" : "lob.enabled",
            "x-category" : "CONNECTOR_ADVANCED"
          },
          "schema.history.internal.kafka.recovery.attempts" : {
            "format" : "int32",
            "title" : "Max attempts to recovery database schema history",
            "description" : "The number of attempts in a row that no data are returned from Kafka before recover completes. The maximum amount of time to wait after receiving no data is (recovery.attempts) x (recovery.poll.interval.ms).",
            "default" : 100,
            "type" : "integer",
            "x-name" : "schema.history.internal.kafka.recovery.attempts",
            "x-category" : "ADVANCED"
          },
          "schema.history.internal.kafka.recovery.poll.interval.ms" : {
            "format" : "int32",
            "title" : "Poll interval during database schema history recovery (ms)",
            "description" : "The number of milliseconds to wait while polling for persisted data during recovery.",
            "default" : 100,
            "type" : "integer",
            "x-name" : "schema.history.internal.kafka.recovery.poll.interval.ms",
            "x-category" : "ADVANCED"
          },
          "skipped.operations" : {
            "format" : "list,regex",
            "title" : "Skipped Operations",
            "description" : "The comma-separated list of operations to skip during streaming, defined as: 'c' for inserts/create; 'u' for updates; 'd' for deletes, 't' for truncates, and 'none' to indicate nothing skipped. By default, only truncate operations will be skipped.",
            "default" : "t",
            "type" : "string",
            "x-name" : "skipped.operations",
            "x-category" : "ADVANCED"
          },
          "event.processing.failure.handling.mode" : {
            "title" : "Event deserialization failure handling",
            "description" : "Specify how failures during processing of events (i.e. when encountering a corrupted event) should be handled, including: 'fail' (the default) an exception indicating the problematic event and its position is raised, causing the connector to be stopped; 'warn' the problematic event and its position will be logged and the event will be skipped; 'ignore' the problematic event will be skipped.",
            "default" : "fail",
            "enum" : [ "warn", "fail", "ignore", "skip" ],
            "type" : "string",
            "x-name" : "event.processing.failure.handling.mode",
            "x-category" : "ADVANCED"
          },
          "query.fetch.size" : {
            "format" : "int32",
            "title" : "Query fetch size",
            "description" : "The maximum number of records that should be loaded into memory while streaming. A value of '0' uses the default JDBC fetch size.",
            "default" : 0,
            "type" : "integer",
            "x-name" : "query.fetch.size",
            "x-category" : "ADVANCED"
          },
          "max.batch.size" : {
            "format" : "int32",
            "title" : "Change event batch size",
            "description" : "Maximum size of each batch of source records. Defaults to 2048.",
            "default" : 2048,
            "type" : "integer",
            "x-name" : "max.batch.size",
            "x-category" : "ADVANCED"
          },
          "max.queue.size" : {
            "format" : "int32",
            "title" : "Change event buffer size",
            "description" : "Maximum size of the queue for change events read from the database log but not yet recorded or forwarded. Defaults to 8192, and should always be larger than the maximum batch size.",
            "default" : 8192,
            "type" : "integer",
            "x-name" : "max.queue.size",
            "x-category" : "ADVANCED"
          },
          "max.queue.size.in.bytes" : {
            "format" : "int64",
            "title" : "Change event buffer size in bytes",
            "description" : "Maximum size of the queue in bytes for change events read from the database log but not yet recorded or forwarded. Defaults to 0. Mean the feature is not enabled",
            "default" : 0,
            "type" : "integer",
            "x-name" : "max.queue.size.in.bytes",
            "x-category" : "ADVANCED"
          },
          "poll.interval.ms" : {
            "format" : "int64",
            "title" : "Poll interval (ms)",
            "description" : "Time to wait for new change events to appear after receiving no events, given in milliseconds. Defaults to 500 ms.",
            "default" : 500,
            "type" : "integer",
            "x-name" : "poll.interval.ms",
            "x-category" : "ADVANCED"
          },
          "retriable.restart.connector.wait.ms" : {
            "format" : "int64",
            "title" : "Retriable restart wait (ms)",
            "description" : "Time to wait before restarting connector after retriable exception occurs. Defaults to 10000ms.",
            "default" : 10000,
            "type" : "integer",
            "x-name" : "retriable.restart.connector.wait.ms",
            "x-category" : "ADVANCED"
          },
          "signal.data.collection" : {
            "title" : "Signaling data collection",
            "description" : "The name of the data collection that is used to send signals/commands to Debezium. Signaling is disabled when not set.",
            "type" : "string",
            "x-name" : "signal.data.collection",
            "x-category" : "ADVANCED"
          },
          "topic.naming.strategy" : {
            "format" : "class",
            "title" : "Topic naming strategy class",
            "description" : "The name of the TopicNamingStrategy class that should be used to determine the topic name for data change, schema change, transaction, heartbeat event etc.",
            "default" : "io.debezium.schema.SchemaTopicNamingStrategy",
            "type" : "string",
            "x-name" : "topic.naming.strategy",
            "x-category" : "ADVANCED"
          },
          "log.mining.buffer.drop.on.stop" : {
            "title" : "Controls whether the buffer cache is dropped when connector is stopped",
            "description" : "When set to true the underlying buffer cache is not retained when the connector is stopped. When set to false (the default), the buffer cache is retained across restarts.",
            "default" : false,
            "type" : "boolean",
            "x-name" : "log.mining.buffer.drop.on.stop",
            "x-category" : "ADVANCED"
          }
        },
        "additionalProperties" : true,
        "x-connector-id" : "oracle",
        "x-version" : "2.1.1.Final",
        "x-className" : "io.debezium.connector.oracle.OracleConnector"
      }
    }
  }
}